This assignment explores how different vision models—CNNs, Vision Transformers, generative models (VAEs, GANs), and multimodal models like CLIP—exhibit distinct inductive biases. These biases shape whether models rely more on texture vs. shape cues, handle local vs. global context, or learn smooth vs. sharp latent representations. Experiments focus on testing these biases through tasks like translation/permutation sensitivity, shape–texture recognition, and robustness to out-of-distribution (OOD) data. CLIP is also studied for its contrastive, semantic bias and zero-shot generalization. The overall goal is to analyze how inductive biases impact both in-distribution performance and OOD robustness.